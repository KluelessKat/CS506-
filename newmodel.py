# -*- coding: utf-8 -*-
"""newmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SokgeZMGcy7pEdYdmot_VO8rkiIXmAIB
"""

import pickle
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

!pip install catboost

# Load the data
data = pd.read_csv('results.csv')

# Drop any rows with missing values
# data = data.dropna()

# Separate features (X) and target variable (y)
X = data.drop(columns=['out_of_pocket', 'family_income', 'UID'])
y = np.log(data['out_of_pocket']+1)

label_encoder = LabelEncoder()

# Encode the 'cancer_type' column
X['cancer_type'] = label_encoder.fit_transform(X['cancer_type'])

label_encoder = LabelEncoder()
X['insurance_type'] = label_encoder.fit_transform(X['insurance_type'])

# Split the dataset (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train

# Define models
models = {
    #"RandomForest": RandomForestRegressor(random_state=42, n_estimators=300),
    "Catboost": CatBoostRegressor(random_state=42, n_estimators=300, learning_rate=0.02),
    "XGBoost": XGBRegressor(random_state=42, n_estimators=300, objective='reg:squarederror', learning_rate=0.02),
    "LightGBM": LGBMRegressor(random_state=42, n_estimators=300, learning_rate=0.02),
}

# Define tolerance for custom accuracy
tolerance = 0.1  # 10% tolerance

# Function to calculate custom accuracy
def custom_accuracy(y_true, y_pred, tolerance):
    return np.mean(np.abs(y_true - y_pred) / y_true < tolerance) * 100

# Initialize dictionary to store results
model_scores = {}

# Evaluate each model
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    y_pred_train = model.predict(X_train)
    print(f"{model_name} - Train RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train))}")

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Calculate performance metrics
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    custom_acc = custom_accuracy(y_test, y_pred, tolerance)

    model_scores[model_name] = {"RMSE": rmse, "MAE": mae, "Custom Accuracy (10% tolerance)": custom_acc}
    print(f"{model_name} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, Custom Accuracy (10% tolerance): {custom_acc:.2f}%")

# Display model performance comparison
model_performance = pd.DataFrame(model_scores).T
print("\nModel Performance Comparison:\n", model_performance)

# Optional: Plot model performance
model_performance.plot(kind='bar', figsize=(10, 6))
plt.title("Model Performance (RMSE and MAE)")
plt.ylabel("Error")
plt.show()

# Save the best model based on RMSE
best_model_name = model_performance['RMSE'].idxmin()
best_model = models[best_model_name]
pickle.dump(best_model, open(f'{best_model_name}_model.pkl', 'wb'))
print(f"Best model ({best_model_name}) saved as '{best_model_name}_model.pkl'.")

# Check feature importance using Random Forest
feature_importances = models["Catboost"].feature_importances_
important_features = pd.Series(feature_importances, index=X.columns).sort_values(ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
important_features.plot(kind='bar')
plt.title("Feature Importance (Catboost)")
plt.show()

from sklearn.model_selection import RandomizedSearchCV
from lightgbm import LGBMRegressor

# Check correlations
correlation_matrix = X.corr()
sns.heatmap(correlation_matrix, cmap="coolwarm", annot=False)
plt.title("Feature Correlation Heatmap")
plt.show()

# from sklearn.model_selection import train_test_split

# # Split data into train and test sets *before* applying transformations
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Impute missing values using SimpleImputer before applying PolynomialFeatures
# from sklearn.impute import SimpleImputer
# imputer = SimpleImputer(strategy='mean') # You can choose a different strategy if needed

# # Fit and transform on training data, then transform test data
# X_train_imputed = imputer.fit_transform(X_train)
# X_test_imputed = imputer.transform(X_test)

# from sklearn.model_selection import GridSearchCV

# # Parameter grid for Random Forest
# param_grid = {
#     'n_estimators': [100, 300],
#     'max_depth': [10, 20],
#     'min_samples_split': [2, 5],
#     'min_samples_leaf': [1, 2]
# }

# # Initialize and fit GridSearch
# grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5,
#                            scoring='neg_mean_squared_error', n_jobs=-1)
# grid_search.fit(X_train_poly, y_train)

# # Best parameters
# print("Best Parameters for RandomForest:", grid_search.best_params_)

# # Re-evaluate with best model
# best_rf = grid_search.best_estimator_
# y_pred_rf = best_rf.predict(X_test)
# print("Tuned RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

from lightgbm import LGBMRegressor
from sklearn.model_selection import GridSearchCV

param_grid_lgbm = {
    'n_estimators': [100, 300],      # Fewer boosting rounds to test
    'max_depth': [10, -1],           # Moderate depth and no limit
    'num_leaves': [20, 31],          # Smaller number of leaves
    'learning_rate': [0.01, 0.05],   # Common learning rates
    'subsample': [0.8, 1.0],         # Sample fraction for bagging
    'colsample_bytree': [0.8, 1.0]   # Feature sampling
}

grid_search_lgbm = GridSearchCV(LGBMRegressor(random_state=42), param_grid_lgbm, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_lgbm.fit(X_train, y_train)

print("Best Parameters for LightGBM:", grid_search_lgbm.best_params_)

import lightgbm as lgb
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

best_params = {
    'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 300, 'num_leaves': 31, 'subsample': 0.8
}

# Initialize LightGBM model with the best parameters
best_lgbm = lgb.LGBMRegressor(**best_params)

# Fit the model to the training data
best_lgbm.fit(X_train, y_train)

# Predict on the test set
y_pred_lgbm = best_lgbm.predict(X_test)

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))
print("Tuned RMSE for LightGBM:", rmse)

# Calculate R-squared (RÂ²) for accuracy
r2 = r2_score(y_test, y_pred_lgbm)
print("R-squared for LightGBM:", r2)